"""
Fresh PDF Processing Script for Healthcare Chatbot
This script processes PDF files from scratch and creates a fresh Pinecone index
"""
from dotenv import load_dotenv
import os
from src.helper import load_pdf_file, filter_to_minimal_docs, text_split, download_hugging_face_embeddings
from pinecone import Pinecone
from pinecone import ServerlessSpec 
from langchain_pinecone import PineconeVectorStore

# Load environment variables
load_dotenv()

PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')
os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY

print("ğŸš€ Starting Fresh PDF Processing...")
print("=" * 50)

# Step 1: Load PDF files
print("ğŸ“š Step 1: Loading PDF files from data/ directory...")
try:
    extracted_data = load_pdf_file(data='data/')
    print(f"âœ… Successfully loaded {len(extracted_data)} documents")
    
    # Show some stats about the loaded documents
    total_chars = sum(len(doc.page_content) for doc in extracted_data)
    print(f"ğŸ“Š Total characters: {total_chars:,}")
    print(f"ğŸ“„ Average characters per document: {total_chars // len(extracted_data):,}")
except Exception as e:
    print(f"âŒ Error loading PDF files: {e}")
    exit(1)

# Step 2: Filter to minimal documents
print("\nğŸ” Step 2: Filtering to minimal documents...")
try:
    filter_data = filter_to_minimal_docs(extracted_data)
    print(f"âœ… Filtered to {len(filter_data)} clean documents")
except Exception as e:
    print(f"âŒ Error filtering documents: {e}")
    exit(1)

# Step 3: Split into text chunks
print("\nâœ‚ï¸ Step 3: Splitting text into chunks...")
try:
    text_chunks = text_split(filter_data)
    print(f"âœ… Created {len(text_chunks)} text chunks")
    
    # Show chunk statistics
    chunk_sizes = [len(chunk.page_content) for chunk in text_chunks]
    avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)
    print(f"ğŸ“Š Average chunk size: {avg_chunk_size:.0f} characters")
    print(f"ğŸ“Š Chunk size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters")
except Exception as e:
    print(f"âŒ Error splitting text: {e}")
    exit(1)

# Step 4: Initialize embeddings
print("\nğŸ§  Step 4: Initializing HuggingFace embeddings...")
try:
    embeddings = download_hugging_face_embeddings()
    print("âœ… Successfully initialized sentence-transformers/all-MiniLM-L6-v2 embeddings")
    print("ğŸ“ Embedding dimension: 384")
except Exception as e:
    print(f"âŒ Error initializing embeddings: {e}")
    exit(1)

# Step 5: Connect to Pinecone and manage index
print("\nğŸŒ² Step 5: Managing Pinecone index...")
try:
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index_name = "medical-chatbot"
    
    # Check if index exists and delete it for fresh start
    if pc.has_index(index_name):
        print(f"ğŸ—‘ï¸ Deleting existing index '{index_name}' for fresh start...")
        pc.delete_index(index_name)
        print("âœ… Existing index deleted")
    
    # Create fresh index
    print(f"ğŸ†• Creating fresh index '{index_name}'...")
    pc.create_index(
        name=index_name,
        dimension=384,  # sentence-transformers/all-MiniLM-L6-v2 dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1"),
    )
    print("âœ… Fresh index created successfully")
    
except Exception as e:
    print(f"âŒ Error managing Pinecone index: {e}")
    exit(1)

# Step 6: Create and populate vector store
print("\nğŸ“¥ Step 6: Creating vector store and uploading embeddings...")
try:
    print("â³ This may take a few minutes to process all chunks...")
    
    docsearch = PineconeVectorStore.from_documents(
        documents=text_chunks,
        index_name=index_name,
        embedding=embeddings, 
    )
    
    print("âœ… Vector store created successfully!")
    print(f"ğŸ“Š Uploaded {len(text_chunks)} document chunks to Pinecone")
    
except Exception as e:
    print(f"âŒ Error creating vector store: {e}")
    exit(1)

# Step 7: Test the vector store
print("\nğŸ§ª Step 7: Testing vector store with sample query...")
try:
    test_query = "What are the symptoms of diabetes?"
    results = docsearch.similarity_search(test_query, k=3)
    
    print(f"âœ… Test query successful!")
    print(f"ğŸ“ Query: '{test_query}'")
    print(f"ğŸ“Š Retrieved {len(results)} relevant documents")
    
    for i, result in enumerate(results, 1):
        preview = result.page_content[:100].replace('\n', ' ')
        print(f"  {i}. {preview}...")
        
except Exception as e:
    print(f"âŒ Error testing vector store: {e}")
    exit(1)

print("\n" + "=" * 50)
print("ğŸ‰ FRESH PDF PROCESSING COMPLETED SUCCESSFULLY!")
print("=" * 50)
print("ğŸ“‹ Summary:")
print(f"   â€¢ Processed: {len(extracted_data)} PDF pages")
print(f"   â€¢ Created: {len(text_chunks)} text chunks")
print(f"   â€¢ Index: '{index_name}' (fresh)")
print(f"   â€¢ Embeddings: 384-dimensional vectors")
print(f"   â€¢ Ready for: Healthcare chatbot with Groq API")
print("\nğŸš€ You can now start the chatbot application!")